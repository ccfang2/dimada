% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dimada.R
\name{dimada}
\alias{dimada}
\title{Dimension Adaptive Estimation with Sieves and Lasso-type Regularization}
\usage{
dimada(
  y,
  x,
  basis = "power",
  n.basis = NULL,
  max.interaction = 1,
  x.sieve = NULL,
  methods = "Lasso",
  family = "gaussian",
  nfolds = 10,
  scale = 1,
  parallel = FALSE,
  lambda.min.ratio = 1e-10,
  s = "lambda.min",
  seed = 200
)
}
\arguments{
\item{y}{a numeric vector of response variable}

\item{x}{a data frame or a matrix containing the original independent variables. The number of observations must be the same as the length of \code{y}. If the argument \code{x.sieve} is given directly, then \code{x} should be \code{NULL}.}

\item{basis}{a character string that specifies the basis with which the sieves of \code{x} are constructed. Available bases in \code{dimada} include \code{"power"}, \code{"legendre"}, \code{"bspline"}, \code{"haar"}, \code{"daubechies"},
\code{"cosine"}, \code{"sine"} and \code{"trig"}. Default is \code{"power"}, i.e., power series. If the argument \code{x.sieve} is given directly, there is no need to define \code{basis}, and one can simply leave it as default.}

\item{n.basis}{a non-negative integer that specifies the number of basis (or the level of basis in cases of haar and daubechies wavelets) for each variable. If \code{NULL}, it will be calculated according to the formulas established in this \href{https://github.com/ccfang2/Masters_Thesis}{thesis}.
Default is \code{NULL}. If the argument \code{x.sieve} is given, there is no need to define \code{n.basis}, and one can just leave it as default.}

\item{max.interaction}{a positive integer that specifies the largest number of interacting variables in a single term of the resulting sieve. For example, \code{max.interaction=1} indicates no interaction among different variables. It shouldn't
be larger than the number of original variables.}

\item{x.sieve}{a data frame or a matrix already containing the sieves of original dataset. If it is given, the arguments \code{x}, \code{basis}, \code{n.basis} and \code{max.interaction} are not needed. The data frame in the output of functions like \code{poly.gen} can be directly used as
an input of \code{x.sieve}. Default is \code{NULL}, when \code{x} must be provided.}

\item{methods}{a character vector that specifies the methods one hopes to use to select significant terms in the sieve. Available methods comprise \code{"Lasso"}, \code{"adaLasso"} and \code{"taLasso"}. Importantly, \code{"Lasso"} is the baseline method which must be included in this argument.
\code{"adaLasso"} stands for \href{https://www.tandfonline.com/doi/abs/10.1198/016214506000000735}{adaptive Lasso}, and \code{"taLasso"} represents \href{https://www.sciencedirect.com/science/article/abs/pii/S030440762100049X}{twin adaptive Lasso}, which is literally post-selection adaptive Lasso.}

\item{family}{a character string that specifies the quantitative family of response variable. Available families include \code{"gaussian"}, \code{"binomial"}, \code{"poisson"}, \code{"multinomial"}, \code{"cox"} and \code{"mgaussian"}. For more explanation of \code{family}, see function
\code{\link[stats]{glm}} in package \pkg{stats}. Default is \code{"gaussian"}.}

\item{nfolds}{an integer that specifies the number of folds in cross validation. It should be between 3 and the number of observations. Cross validation is needed to select the terms of sieves that give the best out-of-sample approximation error in Lasso-type methods. Default is \code{10}.}

\item{scale}{a non-negative number that regulates the weight given to the L1 penalization in adaptive Lasso (also twin adaptive Lasso). According to equation (4) in \href{https://www.tandfonline.com/doi/abs/10.1198/016214506000000735}{Zou (2006)}, the weight is \eqn{\frac{1}{|\hat{\beta}|^{\mathrm{scale}}}}. Hence, if \code{scale=0}, the weight for all \eqn{\beta} is 1,
which reduces adaptive Lasso to Lasso. If \code{scale=1}, the weight for higher \eqn{\beta} is smaller, and this is the key insight of adaptive Lasso. Thus, default value of \code{scale} is \code{1}.}

\item{parallel}{a logical value that indicates if a user hopes to use parallel \code{foreach} to fit each fold. If \code{TRUE}, the user must register parallel beforehand, such as \code{doMC}. See example below. Default is \code{FALSE}.}

\item{lambda.min.ratio}{a ratio that specifies the smallest value for \code{lambda} (i.e., penalty parameter in Lasso-type methods) as a fraction of maximal \code{lambda}.}

\item{s}{a character string that specifies the selection criterion of \code{lambda} in Lasso-type regularization. Available \code{"s"} includes \code{"lambda.min"} and \code{"lambda.1se"}. The former chooses the \code{lambda} that minimizes the cross-validated (i.e., out-of-sample) approximation error,
while the latter selects the largest value of \code{lambda} such that the cross-validated approximation error is within 1 standard error of the minimum. Default is \code{"lambda.min"}.}

\item{seed}{an integer that controls the randomness of cross validation. With all other arguments remain unchanged, the same seed will lead to the same output. \code{seed=NULL} implies no control over the randomness.}
}
\value{
The \code{dimada} command gives out an object of S3 class \code{"dimada"}, which is a list of results from all methods defined in the argument \code{methods} above. The result generally consists of the following items.
\itemize{
\item \code{sieve}: a data frame containing the response variable and the sieves that are either generated from \code{x} or directly given by \code{x.sieve}.
\item \code{method}: a character string that denotes the method, i.e., "Lasso", "adaLasso" or "taLasso".
\item \code{parameters}: a data frame containing all lambdas (i.e., penalty parameters) that are used in above \code{method} and the consequent cross-validated approximation errors (i.e., mean squared errors) as well as corresponding numbers of non zero coefficients of terms in the sieve.
\item \code{parameters.final}: a data frame containing the selected lambda and the corresponding cross-validated approximation error and number of non-zero coefficients. The selection rule is defined in argument \code{s}.
\item \code{coefs.final}: a data frame containing the selected terms in the sieve and their coefficients.
\item \code{lasso.all}, \code{adaLasso.all} or \code{taLasso.all}: an object of S3 class \code{"cv.glmnet"} containing all Lasso-type models estimated with a full list of lambdas as presented in \code{parameters}.
\item \code{elapse}: time used to estimate all models with the above \code{method}. It is noted that "adaLasso" is based on the result of "Lasso", and "taLasso" is based on the result of "adaLasso", so time used for the methods of "Lasso", "adaLasso" and "taLasso" increases accordingly.
\item \code{post.lm}: an object of S3 class \code{"lm"} containing the OLS model with all selected terms from Lasso-type methods.
}
Attributes are returned that correspond to the arguments \code{basis}, \code{n.basis}, \code{max.interaction}, \code{methods} and \code{s}. Also, if \code{x.sieve} is not given directly, the time used to compute sieves for \code{x} is then included as \code{sieve.time} in attributes. However, if \code{x.sieve} is given,
the attributes of \code{basis}, \code{n.basis}, \code{max.interaction} and \code{sieve.time} are \code{NA}.
}
\description{
The \code{dimada} command implements dimension adaptive estimation with sieves and Lasso-type methods as proposed in this \href{https://github.com/ccfang2/Masters_Thesis}{thesis}.
The key advantage of dimension adaptive estimator is that its rate of convergence is adaptive to the dimension of underlying true model, which is usually unknown. Specifically, if the underlying
model is parametric, the convergence rate of this estimator is as fast as parametric estimator (e.g., OLS); if the underlying model is non-parametric, the convergence rate of this estimator is
slower but it still converges whilst the usual parametric method doesn't converge at all. To put it simply, the dimension adaptive estimator is weakly dominant in terms of approximation
error across different underlying models. One can implement this estimator easily with this \code{dimada} command.
}
\note{
The function \code{\link[glmnet]{cv.glmnet}} in package \pkg{glmnet} is applied to conduct Lasso-type methods in \code{dimada}. Cross-validation is demanded because out-of-sample approximation errors are needed to select the best model, as required in the dimension adaptive estimator.

Moreover, post-selection OLS model is performed because the approximation error in Lasso-type methods also includes a regularization term which is always non-negative. Thus, it would be better to run a post-selection model without a regularization term so that the estimation result can be directly compared with other estimators.
}
\examples{
# a data frame with 3 independent variables
set.seed(200)
df <- data.frame(a=runif(100), b=runif(100), c=runif(100))
# simulate a response variable with a parametric model with coefficients of c(1,2,1)
# and a normal error term
response1 <- as.matrix(df)\%*\%c(1,2,1)+rnorm(100,0.04)
# dimension adaptive estimation with power series and full interactions among independent
# variables as well as methods of Lasso and adaptive Lasso
dimada1 <- dimada(y=as.vector(response1), x=df, basis="power", max.interaction=3,
                  methods=c("Lasso","adaLasso"))

# simulate a response variable with a non-parametric additive model and a normal error term
set.seed(200)
response2 <- sin(2*df[,1])+tan(df[,2])+log(df[,3])+rnorm(100,0.04)
# dimension adaptive estimation with B-splines and no interactions among variables
dimada2 <- dimada(y=response2, x=df, basis="bspline", max.interaction=1,
                  methods=c("Lasso","adaLasso"))

# the same as above, but compute sieve with function 'bspline.gen' at first and then use
# the output as an input of 'x.sieve' in function 'dimada' directly
sieve <- bspline.gen(data=df, max.interaction=1)$train
dimada3 <- dimada(y=response2, x=NULL, x.sieve=sieve, methods=c("Lasso","adaLasso"))

# Parallel
# install.packages("doMC")
require(doMC)
registerDoMC(cores = 2)
system.time(dimada(y=response2, x=df, basis="bspline", max.interaction=1,
            methods=c("Lasso","adaLasso")))
system.time(dimada(y=response2, x=df, basis="bspline", max.interaction=1,
            methods=c("Lasso","adaLasso"), parallel=TRUE))
}
\seealso{
\link{dimada.plot}; \link{dimada.summary}; \link{poly.gen}; \link{bspline.gen}; \link{cosine.gen}; \link{sine.gen}; \link{trig.gen}; \link{haar.gen}; \link{daubechies.gen}.
}
\author{
Chencheng Fang, Bonn Graduate School of Economics, University of Bonn. Email: \email{ccfang@uni-bonn.de}
}
